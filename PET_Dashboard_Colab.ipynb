{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801ac5d3",
   "metadata": {},
   "source": [
    "# 🚀 PET Resource Allocation Dashboard - Google Colab Deployment\n",
    "\n",
    "This notebook will set up and run your PET Resource Allocation Dashboard in Google Colab with public access via ngrok.\n",
    "\n",
    "## 📋 Instructions:\n",
    "1. **Run all cells in order** (Runtime → Run all)\n",
    "2. **Upload your CSV data** when prompted\n",
    "3. **Get public URL** from the final cell\n",
    "4. **Share the URL** with your team\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56359acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 STEP 1: Install Dependencies\n",
    "print(\"🔧 Installing required packages...\")\n",
    "\n",
    "!pip install -q streamlit pandas plotly openpyxl pyngrok python-dateutil numpy watchdog\n",
    "\n",
    "print(\"✅ Dependencies installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 STEP 2: Create Directory Structure\n",
    "print(\"📁 Setting up project structure...\")\n",
    "\n",
    "import os\n",
    "os.makedirs('src/components', exist_ok=True)\n",
    "os.makedirs('src/views', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print(\"✅ Directory structure created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b0754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 STEP 3: Create Core Schema Module\n",
    "print(\"📝 Creating schema module...\")\n",
    "\n",
    "schema_code = '''\"\"\"\n",
    "Schema definitions and column mappings for PET Resource Allocation Dashboard\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "# Sentinel values for header detection\n",
    "SENTINELS = {\"Supervisor or Hiring Manager\", \"Resource or Rec/Offer\", \"L3 Org\"}\n",
    "\n",
    "# Column mappings from business headers to normalized names\n",
    "HEADER_MAP = {\n",
    "    \"Supervisor or Hiring Manager\": \"manager\",\n",
    "    \"Resource or Rec/Offer\": \"resource_raw\",\n",
    "    \"Type\": \"type\",\n",
    "    \"L3 Org\": \"l3_org\",\n",
    "    \"VP Org\": \"vp_org\",\n",
    "    \"Director Org\": \"director_org\",\n",
    "    \"Total Workstream Allocation %\": \"total_workstream_allocation_pct\",\n",
    "}\n",
    "\n",
    "# Dynamic workstream detection patterns\n",
    "WS_NAME_RE = re.compile(r\"^\\\\s*Workstream\\\\s*(\\\\d+)\\\\s*$\", re.I)\n",
    "PCT_NAME_RE = re.compile(r\"^\\\\s*%+\\\\s*(\\\\d+)\\\\s*$\", re.I)\n",
    "\n",
    "# Type normalization mapping\n",
    "TYPE_MAPPING = {\n",
    "    'employee': 'Employee',\n",
    "    'contractor': 'Contractor',\n",
    "    'req': 'Req',\n",
    "    'open role': 'Open Role',\n",
    "    'request': 'Req',\n",
    "    'requisition': 'Req',\n",
    "}\n",
    "\n",
    "# Allocation status thresholds\n",
    "ALLOCATION_THRESHOLDS = {\n",
    "    'underallocated': 100,\n",
    "    'overallocated': 100,\n",
    "    'unassigned': 0\n",
    "}\n",
    "\n",
    "ALLOCATION_TOLERANCE = 1.0\n",
    "WATCH_PATTERN = r'PET Resource Allocation.*\\\\.csv'\n",
    "'''\n",
    "\n",
    "with open('src/schema.py', 'w') as f:\n",
    "    f.write(schema_code)\n",
    "\n",
    "print(\"✅ Schema module created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19732ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 STEP 4: Create ETL Module (Data Processing)\n",
    "print(\"📊 Creating ETL module...\")\n",
    "\n",
    "etl_code = '''\"\"\"\n",
    "ETL pipeline for PET Resource Allocation data\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Optional, Tuple\n",
    "from .schema import HEADER_MAP, SENTINELS, WS_NAME_RE, PCT_NAME_RE, TYPE_MAPPING, ALLOCATION_TOLERANCE\n",
    "\n",
    "def _likely_header_row(row_vals) -> bool:\n",
    "    \"\"\"Check if a row likely contains header information\"\"\"\n",
    "    vals = {str(v).strip() for v in row_vals if pd.notna(v)}\n",
    "    # require at least 2 sentinel hits to avoid false positives\n",
    "    return len(SENTINELS.intersection(vals)) >= 2\n",
    "\n",
    "def read_with_embedded_header(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read CSV with embedded header detection (handles both embedded and standard formats)\"\"\"\n",
    "    # 1) read raw with no header first\n",
    "    raw = pd.read_csv(csv_path, header=None, dtype=str, keep_default_na=False)\n",
    "    \n",
    "    # 2) Check if first row has business headers (normal CSV)\n",
    "    if len(raw) > 0 and _likely_header_row(raw.iloc[0].tolist()):\n",
    "        # Standard format - headers in first row\n",
    "        df = pd.read_csv(csv_path, header=0, dtype=str, keep_default_na=False)\n",
    "        return df\n",
    "    \n",
    "    # 3) Look for embedded headers in first 10 rows\n",
    "    header_idx = None\n",
    "    scan_rows = min(10, len(raw))\n",
    "    for i in range(scan_rows):\n",
    "        if _likely_header_row(raw.iloc[i].tolist()):\n",
    "            header_idx = i\n",
    "            break\n",
    "    \n",
    "    # 4) For files with embedded headers, use the ORIGINAL column structure\n",
    "    # but skip the business header row and any empty rows\n",
    "    if header_idx is not None:\n",
    "        # Use original pandas column names, skip to data after business headers\n",
    "        df = pd.read_csv(csv_path, header=0, dtype=str, keep_default_na=False)\n",
    "        # Skip the business header row (it becomes the first data row)\n",
    "        if len(df) > header_idx:\n",
    "            df = df.iloc[header_idx:].copy()\n",
    "            # Remove the business header row itself\n",
    "            if len(df) > 0:\n",
    "                df = df.iloc[1:].copy()\n",
    "        return df\n",
    "    \n",
    "    # 5) Fallback: try reading as normal CSV \n",
    "    df = pd.read_csv(csv_path, header=0, dtype=str, keep_default_na=False)\n",
    "    return df\n",
    "\n",
    "def find_workstream_pairs(cols):\n",
    "    \"\"\"Dynamically find workstream and percentage column pairs\"\"\"\n",
    "    # Pattern 1: Look for \"Workstream N\" and \"% N\" pairs (legacy format)\n",
    "    ws_cols = {}\n",
    "    pct_cols = {}\n",
    "    for c in cols:\n",
    "        m1 = WS_NAME_RE.match(str(c))\n",
    "        m2 = PCT_NAME_RE.match(str(c))\n",
    "        if m1:\n",
    "            ws_cols[int(m1.group(1))] = c\n",
    "        elif m2:\n",
    "            pct_cols[int(m2.group(1))] = c\n",
    "    \n",
    "    # Pattern 2: Look for \"Workstream N\" and \"Workstream N.2\" pairs (new format)\n",
    "    if not pct_cols:  # If no % columns found, try the new format\n",
    "        ws_name_pattern = re.compile(r\"^\\\\s*Workstream\\\\s*(\\\\d+)\\\\s*$\", re.I)\n",
    "        ws_pct_pattern = re.compile(r\"^\\\\s*Workstream\\\\s*(\\\\d+)\\\\.2\\\\s*$\", re.I)\n",
    "        \n",
    "        for c in cols:\n",
    "            m1 = ws_name_pattern.match(str(c))\n",
    "            m2 = ws_pct_pattern.match(str(c))\n",
    "            if m1:\n",
    "                ws_cols[int(m1.group(1))] = c\n",
    "            elif m2:\n",
    "                pct_cols[int(m2.group(1))] = c\n",
    "    \n",
    "    pairs = []\n",
    "    for k in sorted(set(ws_cols) & set(pct_cols)):\n",
    "        pairs.append((ws_cols[k], pct_cols[k]))\n",
    "    return pairs\n",
    "\n",
    "def parse_employee(raw):\n",
    "    \"\"\"Parse employee information from raw resource string\"\"\"\n",
    "    # \"72633: Senior Software Engineer - Jane Doe\" or similar\n",
    "    if not isinstance(raw, str): \n",
    "        return None, None, None\n",
    "    parts = raw.split(':', 1)\n",
    "    if len(parts) == 2:\n",
    "        emp_id = re.sub(r'\\\\D', '', parts[0]) or None\n",
    "        title = parts[1].strip()\n",
    "        name = title.split(' - ')[-1].strip() if ' - ' in title else title\n",
    "        return emp_id, name, title\n",
    "    return None, raw.strip(), raw.strip()\n",
    "\n",
    "def compute_total_allocation(df):\n",
    "    \"\"\"Compute total allocation with tolerance for missing Total column\"\"\"\n",
    "    pairs = find_workstream_pairs(df.columns)\n",
    "    if not pairs:\n",
    "        df[\"computed_total_pct\"] = 0.0\n",
    "        df[\"total_allocation_pct\"] = df.get(\"total_workstream_allocation_pct\", pd.Series([None]*len(df)))\n",
    "        return df\n",
    "\n",
    "    comp = None\n",
    "    for _, pct_col in pairs:\n",
    "        col = pd.to_numeric(df[pct_col], errors=\"coerce\")\n",
    "        if col.dropna().between(0,1).mean() > 0.85:  # scale\n",
    "            col = col * 100.0\n",
    "        comp = col if comp is None else comp.add(col, fill_value=0)\n",
    "    df[\"computed_total_pct\"] = comp.fillna(0)\n",
    "    canon = \"Total Workstream Allocation %\"\n",
    "    if canon in df.columns:\n",
    "        df[\"total_allocation_pct\"] = pd.to_numeric(df[canon], errors=\"coerce\").fillna(df[\"computed_total_pct\"])\n",
    "    else:\n",
    "        df[\"total_allocation_pct\"] = df[\"computed_total_pct\"]\n",
    "    return df\n",
    "\n",
    "def load_latest_csv(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"End-to-end CSV loader with header detection\"\"\"\n",
    "    df = read_with_embedded_header(csv_path)\n",
    "    # rename canonical business columns to code-friendly names\n",
    "    rename_map = {\n",
    "        \"Supervisor or Hiring Manager\": \"manager\",\n",
    "        \"Resource or Rec/Offer\": \"resource_raw\",\n",
    "        \"Type\": \"type\",\n",
    "        \"L3 Org\": \"l3_org\",\n",
    "        \"VP Org\": \"vp_org\",\n",
    "        \"Director Org\": \"director_org\",\n",
    "        \"Total Workstream Allocation %\": \"total_workstream_allocation_pct\",\n",
    "    }\n",
    "    for k,v in rename_map.items():\n",
    "        if k in df.columns:\n",
    "            df = df.rename(columns={k: v})\n",
    "        else:\n",
    "            df[v] = \"\"\n",
    "\n",
    "    # parse resource fields\n",
    "    parsed = df[\"resource_raw\"].apply(parse_employee)\n",
    "    df[[\"employee_id\",\"resource_name\",\"resource_title\"]] = pd.DataFrame(parsed.tolist(), index=df.index)\n",
    "\n",
    "    # totals\n",
    "    df = compute_total_allocation(df)\n",
    "\n",
    "    # flags\n",
    "    df[\"overallocated\"] = df[\"total_allocation_pct\"] > 100.0\n",
    "    df[\"underallocated\"] = df[\"total_allocation_pct\"] < 100.0\n",
    "    df[\"unassigned\"] = df[\"total_allocation_pct\"].fillna(0) == 0.0\n",
    "    return df\n",
    "\n",
    "def unpivot_assignments(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create workstream assignments DataFrame\"\"\"\n",
    "    pairs = find_workstream_pairs(df.columns)\n",
    "    rows = []\n",
    "    base_cols = [\"resource_raw\",\"type\",\"manager\",\"director_org\",\"vp_org\",\"l3_org\"]\n",
    "    # Add resource_name if available\n",
    "    if \"resource_name\" in df.columns:\n",
    "        base_cols.append(\"resource_name\")\n",
    "    \n",
    "    for ws_col, pct_col in pairs:\n",
    "        part = df[base_cols + [ws_col, pct_col]].copy()\n",
    "        part = part.rename(columns={ws_col: \"workstream\", pct_col: \"allocation_pct\"})\n",
    "        # numeric & scaling\n",
    "        part[\"allocation_pct\"] = pd.to_numeric(part[\"allocation_pct\"], errors=\"coerce\")\n",
    "        # auto-scale if mostly 0–1\n",
    "        if part[\"allocation_pct\"].dropna().between(0,1).mean() > 0.85:\n",
    "            part[\"allocation_pct\"] = part[\"allocation_pct\"] * 100.0\n",
    "        part = part[part[\"allocation_pct\"].fillna(0) > 0]\n",
    "        rows.append(part)\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=[\"workstream\",\"allocation_pct\"])\n",
    "\n",
    "def process_pet_csv(file_path: str):\n",
    "    \"\"\"Main function to process PET CSV file\"\"\"\n",
    "    try:\n",
    "        # Use the new header-aware loader\n",
    "        df = load_latest_csv(file_path)\n",
    "        \n",
    "        # Create workstream assignments using the new unpivot function\n",
    "        workstream_df = unpivot_assignments(df)\n",
    "        \n",
    "        print(f\"Processed {len(df)} resources with {len(workstream_df)} workstream assignments\")\n",
    "        \n",
    "        return df, workstream_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV {file_path}: {str(e)}\")\n",
    "        raise\n",
    "'''\n",
    "\n",
    "with open('src/etl.py', 'w') as f:\n",
    "    f.write(etl_code)\n",
    "\n",
    "print(\"✅ ETL module created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c573c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🗃️ STEP 5: Create Store and Component Modules\n",
    "print(\"🗃️ Creating remaining modules...\")\n",
    "\n",
    "# Create __init__.py files\n",
    "with open('src/__init__.py', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "with open('src/components/__init__.py', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "with open('src/views/__init__.py', 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "# Create simplified store module\n",
    "store_code = '''\"\"\"\n",
    "Data storage and file handling\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from .etl import process_pet_csv\n",
    "\n",
    "def load_latest_data(data_dir=\"data\"):\n",
    "    \"\"\"Load the most recent PET CSV file\"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Find CSV files matching pattern\n",
    "    csv_files = list(data_path.glob(\"*PET*Resource*Allocation*.csv\"))\n",
    "    if not csv_files:\n",
    "        csv_files = list(data_path.glob(\"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        return None, None\n",
    "    \n",
    "    # Get most recent file\n",
    "    latest_file = max(csv_files, key=lambda f: f.stat().st_mtime)\n",
    "    \n",
    "    try:\n",
    "        df, workstream_df = process_pet_csv(str(latest_file))\n",
    "        return df, workstream_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def get_data_store():\n",
    "    \"\"\"Simple data store getter\"\"\"\n",
    "    return {\"data_dir\": \"data\"}\n",
    "'''\n",
    "\n",
    "with open('src/store.py', 'w') as f:\n",
    "    f.write(store_code)\n",
    "\n",
    "print(\"✅ Store module created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484abcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 STEP 6: Upload Your Data Files\n",
    "print(\"📁 Upload your PET Resource Allocation CSV files...\")\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Please upload your CSV files using the file browser:\")\n",
    "print(\"1. Click the folder icon on the left sidebar\")\n",
    "print(\"2. Navigate to the 'data' folder\")\n",
    "print(\"3. Upload your PET Resource Allocation CSV files\")\n",
    "print(\"\")\n",
    "print(\"Or use the upload button below:\")\n",
    "\n",
    "# File upload widget\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded files to data directory\n",
    "for filename, content in uploaded.items():\n",
    "    if filename.endswith('.csv'):\n",
    "        with open(f'data/{filename}', 'wb') as f:\n",
    "            f.write(content)\n",
    "        print(f\"✅ Uploaded: {filename}\")\n",
    "\n",
    "print(\"✅ Data upload completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e056c75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad84a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎛️ STEP 7: Create Streamlit Application\n",
    "print(\"🎛️ Creating Streamlit application...\")\n",
    "\n",
    "app_code = '''\"\"\"\n",
    "PET Resource Allocation Dashboard - Streamlit Application\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our modules\n",
    "from src.store import load_latest_data, get_data_store\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"PET Resource Allocation Dashboard\",\n",
    "    page_icon=\"🏢\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .stMetric {\n",
    "        background-color: #f0f2f6;\n",
    "        padding: 10px;\n",
    "        border-radius: 5px;\n",
    "    }\n",
    "    .main-header {\n",
    "        background: linear-gradient(90deg, #1f77b4, #ff7f0e);\n",
    "        color: white;\n",
    "        padding: 1rem;\n",
    "        border-radius: 10px;\n",
    "        text-align: center;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "def main():\n",
    "    # Header\n",
    "    st.markdown(\"\"\"\n",
    "    <div class=\"main-header\">\n",
    "        <h1>🏢 PET Resource Allocation Dashboard</h1>\n",
    "        <p>Comprehensive resource allocation analysis and workstream management</p>\n",
    "    </div>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    # Load data\n",
    "    with st.spinner(\"Loading data...\"):\n",
    "        df, workstream_df = load_latest_data(\"data\")\n",
    "    \n",
    "    if df is None:\n",
    "        st.error(\"No data files found. Please upload a PET Resource Allocation CSV file to the data/ folder.\")\n",
    "        st.info(\"Expected file pattern: 'PET Resource Allocation*.csv'\")\n",
    "        return\n",
    "    \n",
    "    # Sidebar filters\n",
    "    st.sidebar.header(\"🔍 Filters\")\n",
    "    \n",
    "    # Type filter\n",
    "    if 'type_normalized' in df.columns:\n",
    "        types = ['All'] + sorted(df['type_normalized'].dropna().unique().tolist())\n",
    "        selected_type = st.sidebar.selectbox(\"Resource Type\", types)\n",
    "        if selected_type != 'All':\n",
    "            df = df[df['type_normalized'] == selected_type]\n",
    "    \n",
    "    # Manager filter\n",
    "    if 'manager' in df.columns:\n",
    "        managers = ['All'] + sorted(df['manager'].dropna().unique().tolist())\n",
    "        selected_manager = st.sidebar.selectbox(\"Manager\", managers)\n",
    "        if selected_manager != 'All':\n",
    "            df = df[df['manager'] == selected_manager]\n",
    "    \n",
    "    # Organization filters\n",
    "    if 'l3_org' in df.columns:\n",
    "        l3_orgs = ['All'] + sorted(df['l3_org'].dropna().unique().tolist())\n",
    "        selected_l3 = st.sidebar.selectbox(\"L3 Organization\", l3_orgs)\n",
    "        if selected_l3 != 'All':\n",
    "            df = df[df['l3_org'] == selected_l3]\n",
    "    \n",
    "    # Main content tabs\n",
    "    tab1, tab2, tab3 = st.tabs([\"📊 Overview\", \"👥 Resources\", \"🎯 Workstreams\"])\n",
    "    \n",
    "    with tab1:\n",
    "        st.header(\"📊 Overview & KPIs\")\n",
    "        \n",
    "        # KPI metrics\n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        \n",
    "        with col1:\n",
    "            total_resources = len(df)\n",
    "            st.metric(\"Total Resources\", total_resources)\n",
    "        \n",
    "        with col2:\n",
    "            total_fte = df['calculated_total'].sum() / 100 if 'calculated_total' in df.columns else 0\n",
    "            st.metric(\"Total FTE\", f\"{total_fte:.1f}\")\n",
    "        \n",
    "        with col3:\n",
    "            if 'type_normalized' in df.columns:\n",
    "                employees = len(df[df['type_normalized'] == 'Employee'])\n",
    "                st.metric(\"Employees\", employees)\n",
    "            else:\n",
    "                st.metric(\"Employees\", \"N/A\")\n",
    "        \n",
    "        with col4:\n",
    "            if 'type_normalized' in df.columns:\n",
    "                contractors = len(df[df['type_normalized'] == 'Contractor'])\n",
    "                st.metric(\"Contractors\", contractors)\n",
    "            else:\n",
    "                st.metric(\"Contractors\", \"N/A\")\n",
    "        \n",
    "        # Allocation status chart\n",
    "        if 'calculated_total' in df.columns:\n",
    "            st.subheader(\"📈 Allocation Status Distribution\")\n",
    "            \n",
    "            # Create allocation status\n",
    "            df['allocation_status'] = df['calculated_total'].apply(\n",
    "                lambda x: 'Overallocated' if x > 100 else 'Underallocated' if x < 100 and x > 0 else 'Unassigned'\n",
    "            )\n",
    "            \n",
    "            status_counts = df['allocation_status'].value_counts()\n",
    "            \n",
    "            fig = px.pie(\n",
    "                values=status_counts.values,\n",
    "                names=status_counts.index,\n",
    "                title=\"Resource Allocation Status\",\n",
    "                color_discrete_map={\n",
    "                    'Overallocated': '#ff6b6b',\n",
    "                    'Underallocated': '#ffd93d',\n",
    "                    'Unassigned': '#6bcf7f'\n",
    "                }\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    with tab2:\n",
    "        st.header(\"👥 Resource Details\")\n",
    "        \n",
    "        # Display main dataframe\n",
    "        if not df.empty:\n",
    "            # Select relevant columns for display\n",
    "            display_cols = []\n",
    "            for col in ['resource_name', 'manager', 'type_normalized', 'l3_org', 'vp_org', 'director_org', 'calculated_total']:\n",
    "                if col in df.columns:\n",
    "                    display_cols.append(col)\n",
    "            \n",
    "            if display_cols:\n",
    "                st.dataframe(\n",
    "                    df[display_cols].head(100),\n",
    "                    use_container_width=True,\n",
    "                    height=400\n",
    "                )\n",
    "            else:\n",
    "                st.dataframe(df.head(100), use_container_width=True, height=400)\n",
    "        \n",
    "        # Download button\n",
    "        if not df.empty:\n",
    "            csv = df.to_csv(index=False)\n",
    "            st.download_button(\n",
    "                label=\"📥 Download Resource Data\",\n",
    "                data=csv,\n",
    "                file_name=f\"pet_resources_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n",
    "                mime=\"text/csv\"\n",
    "            )\n",
    "    \n",
    "    with tab3:\n",
    "        st.header(\"🎯 Workstream Analysis\")\n",
    "        \n",
    "        if workstream_df is not None and not workstream_df.empty:\n",
    "            # Workstream FTE chart\n",
    "            workstream_fte = workstream_df.groupby('workstream')['allocation_pct'].sum() / 100\n",
    "            workstream_fte = workstream_fte.sort_values(ascending=False)\n",
    "            \n",
    "            fig = px.bar(\n",
    "                x=workstream_fte.index,\n",
    "                y=workstream_fte.values,\n",
    "                title=\"FTE Allocation by Workstream\",\n",
    "                labels={'x': 'Workstream', 'y': 'FTE'}\n",
    "            )\n",
    "            fig.update_layout(xaxis_tickangle=-45)\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Workstream details table\n",
    "            st.subheader(\"📋 Workstream Assignments\")\n",
    "            display_ws_cols = ['workstream', 'resource_name', 'allocation_pct', 'manager', 'type']\n",
    "            available_cols = [col for col in display_ws_cols if col in workstream_df.columns]\n",
    "            \n",
    "            if available_cols:\n",
    "                st.dataframe(\n",
    "                    workstream_df[available_cols].sort_values('allocation_pct', ascending=False),\n",
    "                    use_container_width=True,\n",
    "                    height=400\n",
    "                )\n",
    "        else:\n",
    "            st.info(\"No workstream data available.\")\n",
    "\n",
    "    # Footer\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"**PET Resource Allocation Dashboard** | Data automatically refreshed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"✅ Streamlit application created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 STEP 8: Launch Dashboard with Public Access\n",
    "print(\"🚀 Starting PET Dashboard with ngrok tunnel...\")\n",
    "\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "# Set up ngrok (you may need to sign up for a free account at https://ngrok.com)\n",
    "# and set your authtoken: ngrok.set_auth_token(\"YOUR_TOKEN\")\n",
    "\n",
    "def run_streamlit():\n",
    "    \"\"\"Run Streamlit in the background\"\"\"\n",
    "    subprocess.run([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"])\n",
    "\n",
    "# Start Streamlit in a separate thread\n",
    "streamlit_thread = threading.Thread(target=run_streamlit, daemon=True)\n",
    "streamlit_thread.start()\n",
    "\n",
    "# Wait a moment for Streamlit to start\n",
    "time.sleep(10)\n",
    "\n",
    "try:\n",
    "    # Create ngrok tunnel\n",
    "    public_url = ngrok.connect(8501)\n",
    "    print(\"🎉 SUCCESS! Your dashboard is now publicly available at:\")\n",
    "    print(f\"🌐 Public URL: {public_url}\")\n",
    "    print(f\"📱 Share this URL with your team!\")\n",
    "    print(\"\")\n",
    "    print(\"⚡ Dashboard Features:\")\n",
    "    print(\"   • Real-time resource allocation analysis\")\n",
    "    print(\"   • Interactive filters and charts\")\n",
    "    print(\"   • Workstream FTE calculations\")\n",
    "    print(\"   • Data export capabilities\")\n",
    "    print(\"\")\n",
    "    print(\"🔄 The dashboard will continue running...\")\n",
    "    print(\"⏹️ To stop: Runtime → Interrupt execution\")\n",
    "    \n",
    "    # Keep the tunnel alive\n",
    "    while True:\n",
    "        time.sleep(60)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating public tunnel: {e}\")\n",
    "    print(\"\")\n",
    "    print(\"📋 Alternative options:\")\n",
    "    print(\"1. Run locally without public access:\")\n",
    "    print(\"   !streamlit run app.py\")\n",
    "    print(\"\")\n",
    "    print(\"2. Set up ngrok account:\")\n",
    "    print(\"   - Go to: https://ngrok.com/signup\")\n",
    "    print(\"   - Get your authtoken\")\n",
    "    print(\"   - Run: !ngrok authtoken YOUR_TOKEN\")\n",
    "    print(\"   - Then re-run this cell\")\n",
    "    \n",
    "    # Fallback: run streamlit without ngrok\n",
    "    print(\"🔧 Starting local dashboard without public URL...\")\n",
    "    subprocess.run([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
